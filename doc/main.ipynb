{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Composing Music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we aimed to use Long Short Term Memory networks (LSTM) to generate classical and pop music. Here, we treated music as an object across two dimensions (time and note) and recreated a network structure capable of modeling a translationally invariant probability distribution across both time and note axis, conditioned upon information before. After trainning on 174 pieces of classical musics and 153 pieces of pop musics seperately for 50000 iterations, this model is able to produce polyphonic music with chords and noticeable musical structure. Finally, we generated hundreds of pieces of music and selected the most melodious ones, which are attached in the conclusion. We referred blog post “Composing Music with Biaxial RNNs” by Daniel Johnson in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we used biaxial architecture consisting of four layers, which are divided into two stacks, responsibling for time and note. \n",
    "![\"Biaxial architecture consisting of four LSTM\"](./2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs into each stack are rearranged such that the sequential axis is strictly along time or pitch, allowing the LSTM cell group to learn relative structure and conditional distributions along that dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM is a special kind of RNN, which are capable of learning long-term dependencies. Because it can avoid long-term dependency probem, we used it in our model. The LSTM has a chain- structure and there are four neural network layer, interacting with each other in hidden layer. \n",
    "![\"LSTM Structure\"](.\\3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important part for LSTM is that this model can add and remove information to the cell state by gates and sigmoid and tanh neural net layers.   \n",
    "\n",
    "Firstly, when the input message $h_{t-1}$ and $x_t$ walk through the sigmoid layer, which eliminates information that should be forgotten or gated. Here are the forget gate and the input gate: \n",
    "$$f_{t} = \\sigma(W_{f}[h_{t-1},x_{t}]+b_{f})$$\n",
    "$$i_{t}=\\sigma(W_i[h_{t-1},x_{t}]+b_{t})$$\n",
    "\n",
    "Then, the messages walk through a tanh layer and add new information into our cell state:\n",
    "$$\\tilde{C_t}=tanh(W_C[h_{t-1},x_t]+b_C)$$\n",
    "\n",
    "So the gated new input information will merge the old information which is deleted some information, \n",
    "$$C_t=f_t*C_{t-1}+i_t*\\tilde{C_t}$$\n",
    "Finally, we decide the output $h_t$ by tanh and sigmod output gate:\n",
    "$$o_t=\\sigma(W_o[h_{t-1},x_t]+b_o)$$\n",
    "$$h_t=o_t*tanh(C_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the basic idea of LSTM which was used to build our biaxial architecture. And we will metioned the model in the following parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Formation and Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model the joint distribution of all notes conditioned notes before to generate music:\n",
    "$$\\frac{1}{T}\\sum{P(v_t|v_{d})}, d<t-1$$ where $v_t$ is the note vector on time t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to modeling this joint distribution, we also design the network to capture the patterns and structures that are invariant along translations along the note axis. Formally, given $v_i^{(t)}$,  $v_i^{(t+1)}$, $w_i^{(t)}$, $w_i^{(t+1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(v_i^{(t+1)}|v_i^{(t)})=P(w_i^{(t+1)}|w_i^{(t)})$$\n",
    "where $v_i^{(t)}$ is $w_i^{(t)}$ shifted forward along the note axis by d positions, so $v_i^{(t+1)}=w_{i+d}^{(t+1)}$.This means that given two note shifted variant vectors, the network should produce the similarly shifted distribution of outputs, which is an extremely important property of music allowing it to be played in different keys and scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn these relative structures along both the time and note dimensions, a stack of LSTM cells is applied recurrently along each dimension. Thus the biaxial network manifests as a two-tiered architecture, where each tier independently learns the joint sequential distribution along an axis. Since music often contains varying ranges of dependencies, LSTM networks arise as a natural fit for the task due to their invariant nature across the sequential axis, as well as their improved ability to learn long range dependencies during training, thanks to their non-multiplicative errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we need to train our model. As mentioned before, our model is a two-tiered combination of LSTMS along the time and note axis. In each step, the note-axis LSTM layers receive as input of two sources: the activations of the final time-axis LSTM layer for this note, and the final output of the network for the previous note step. The final activations of the note-axis LSTMs will be transformed into a row of two probabilities $p_0^{n,t}$, $p_1^{n,t}$  by the sigmoid function, where $p_0^{n,t}$ is the probability of note n being play at\n",
    "time t , and $p_1^{n,t}$ is the probability of it being articulated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Our loss is then given by the negative log likelihood:\n",
    "$$loss = -\\frac{1}{2TN}\\sum_{t=1}^{T}\\sum_{n=1}^{N}(log(p_0^{n,t}x_0^{n,t}+(1-p_0^{n,t})(1-x_0^{n,t}))+log(p_1^{n,t}x_1^{n,t}+(1-p_1^{n,t})(1-x_1^{n,t}))x_0^{n,t})$$\n",
    "\n",
    "when the note is not played, we can ignore $p_1^{n,t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: Import Package and Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to import functions, which helps us to clean data and train models. And we also need packages, like tensorflow, for us to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../lib')\n",
    "\n",
    "import IPython\n",
    "\n",
    "import model_tb as model\n",
    "import data\n",
    "\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: Train Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know wheteher this model is pre-trained or not. We will check for the pre-trained model, if not, the model will be trained by our model.biaxial_model function. In this step, we generated two models: classical-music model and pop-music model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train on classical music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cache_name = '../output/cache.pkl'\n",
    "model_name = None\n",
    "\n",
    "\n",
    "if not os.path.exists(cache_name):\n",
    "    composers = []\n",
    "    all_pieces = {}\n",
    "\n",
    "    if len(composers)==0:\n",
    "        all_pieces.update(data.getpices(path=\"../data/midis\", mode='all'))\n",
    "    elif composers == 'pop':\n",
    "        all_pieces.update(data.getpices(path='../data/pop_midis', mode='all'))\n",
    "    else:\n",
    "        for c in composers:\n",
    "            all_pieces.update(data.getpices(path=\"../data/midis\", composer=c))\n",
    "\n",
    "    cache = data.initialize_cache(all_pieces, save_loc=cache_name)\n",
    "else:\n",
    "    with open(cache_name, 'rb') as f:\n",
    "        cache = pickle.load(f)\n",
    "\n",
    "# Build and load the pre-existing model if it exists\n",
    "print('Building model')\n",
    "music_model = model.biaxial_model(t_layer_sizes=[300,300],\n",
    "    n_layer_sizes=[100,50],\n",
    "    trainer = tf.train.AdamOptimizer())\n",
    "\n",
    "print('Start training')\n",
    "music_model.train(cache, batch_size=5, max_epoch=50000,\n",
    "    predict_freq=100, pre_trained_model=model_name,save_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train on pop music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cache_name = '../output/pop_cache.pkl'\n",
    "model_name = None\n",
    "\n",
    "\n",
    "if not os.path.exists(cache_name):\n",
    "    composers = 'pop'\n",
    "    all_pieces = {}\n",
    "\n",
    "    if len(composers)==0:\n",
    "        all_pieces.update(data.getpices(path=\"../data/midis\", mode='all'))\n",
    "    elif composers == 'pop':\n",
    "        all_pieces.update(data.getpices(path='../data/pop_midis', mode='all'))\n",
    "    else:\n",
    "        for c in composers:\n",
    "            all_pieces.update(data.getpices(path=\"../data/midis\", composer=c))\n",
    "\n",
    "    cache = data.initialize_cache(all_pieces, save_loc=cache_name)\n",
    "else:\n",
    "    with open(cache_name, 'rb') as f:\n",
    "        cache = pickle.load(f)\n",
    "\n",
    "# Build and load the pre-existing model if it exists\n",
    "print('Building model')\n",
    "music_model = model.biaxial_model(t_layer_sizes=[300,300],\n",
    "    n_layer_sizes=[100,50],\n",
    "    trainer = tf.train.AdamOptimizer())\n",
    "\n",
    "print('Start training')\n",
    "music_model.train(cache, batch_size=5, max_epoch=50000,\n",
    "    predict_freq=100, pre_trained_model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step3: Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first get an initializer seed, sampled from the starting timestep of a measure in a random piece, as the last time step information. This input is fed through one timestep in the time network and then through the entire note dimension recurrently. The output is a complete “chord” distribution for one timestep. This output is then mapped to a new set of input vectors for each note, which is then processed through the biaxial network in a similar fashion. Once a sequence of chords with the desired length has been produced, the outputs are concatenated along the time dimension.\n",
    "\n",
    "The pre-trained models name is biaxial_rnn_1524342232, which is trainned by classical music. We can use this model to  generated music "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cache_name = '../output/pop_cache.pkl'\n",
    "model_name = '../output/model/biaxial_rnn_1524342232' # this model is pre-trained on classical music\n",
    "\n",
    "\n",
    "if not os.path.exists(cache_name):\n",
    "    composers = []\n",
    "    all_pieces = {}\n",
    "\n",
    "    if len(composers)==0:\n",
    "        all_pieces.update(data.getpices(path=\"../data/midis\", mode='all'))\n",
    "    elif composers == 'pop':\n",
    "        all_pieces.update(data.getpices(path='../data/pop_midis', mode='all'))\n",
    "    else:\n",
    "        for c in composers:\n",
    "            all_pieces.update(data.getpices(path=\"../data/midis\", composer=c))\n",
    "\n",
    "    cache = data.initialize_cache(all_pieces, save_loc=cache_name)\n",
    "else:\n",
    "    with open(cache_name, 'rb') as f:\n",
    "        cache = pickle.load(f)\n",
    "\n",
    "# Building model\n",
    "print('Building model')\n",
    "music_model = model.biaxial_model(t_layer_sizes=[300,300], n_layer_sizes=[100,50], \n",
    "                                  trainer=tf.train.AdamOptimizer())\n",
    "\n",
    "print('Start predicting')\n",
    "music_model.predict(cache,model_name,step=320,conservativity=1,n=20,saveto='../output/predict songs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained two models, using classical music and pop music separately. The following selected music is generated by our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio('../doc/classical.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio('../doc/classical_guitar.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio('../doc/pop.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not hard to see that these music have different styles, which is determined by the trained music.To some degree, the result of pop-music model is more melodious than classical-music model, according to the complexity of trained music. However, surprisingly, our classical-music model can play chords. Although the melody is not so mellifluence. But still the generated musice from two model are quite interesting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
